{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dudwo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dudwo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.datasets import load_files\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_exchange_rate_csv(dir):\n",
    "    ret = []\n",
    "    count = 0\n",
    "    currency = []\n",
    "    with open(dir, newline='') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "        for row in spamreader:\n",
    "            tmp = row[0].split(',')\n",
    "            if count>0:\n",
    "                floattmp = []\n",
    "                for val in tmp:\n",
    "                    floattmp.append(float(val))\n",
    "                ret.append(floattmp)\n",
    "            else:\n",
    "                currency = tmp\n",
    "            count += 1\n",
    "    return ret, currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_news(dir, exchange_rates):\n",
    "    X = []; Y=[]\n",
    "    news_dir = dir\n",
    "    for country in os.listdir(news_dir):\n",
    "        print(country)\n",
    "        for week in os.listdir(news_dir+'/'+country):\n",
    "            y = exchange_rates[int(week)][currencies.index(country)]\n",
    "            article = \"\"\n",
    "            for docs in os.listdir(news_dir+'/'+country+'/'+week):\n",
    "                try:\n",
    "                    f = open(news_dir+'/'+country+'/'+week+'/'+docs, 'r', encoding='utf-8')\n",
    "                    article += f.read()\n",
    "                except:\n",
    "                    print('except')\n",
    "                    pass\n",
    "            X.append(article)\n",
    "            Y.append(y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(x):\n",
    "    LUT = {\"ain't\": \"is not\", \"aren't\": \"are not\", \"can't\": \"can not\", \"can't've\": \"can not have\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"couldn't've\": \"could not have\", \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \"he'll've\": \"he he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\", \"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \"i'll've\": \"i will have\", \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\", \"mightn't\": \"might not\", \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\", \"so's\": \"so as\", \"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\", \"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\", \"y'all've\": \"you all have\", \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "    p = re.compile('({})'.format('|'.join(LUT.keys())), flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = LUT.get(match)\\\n",
    "        if LUT.get(match)\\\n",
    "        else LUT.get(match.lower())\n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "    expanded_text = p.sub(expand_match, x)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_processing(X):\n",
    "    stemmer = WordNetLemmatizer()\n",
    "    for i in range(len(X)):\n",
    "        x = re.sub(r'’', \"'\", X[i])\n",
    "        x = re.sub(r'”', '\"', x)\n",
    "        x = re.sub(r'\\\\n', ' ', x)\n",
    "        x = re.sub(r'\\\\\\'', \"'\", x)\n",
    "        x = expand_contractions(x)\n",
    "\n",
    "        x = re.sub(r'[^a-zA-Z0-9\\s]', ' ', x)\n",
    "        x = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', x)\n",
    "        x = re.sub(r'\\^[a-zA-Z]\\s+', ' ', x)\n",
    "        x = re.sub(r'\\s+', ' ', x, flags=re.I)\n",
    "\n",
    "        x = x.lower()\n",
    "        x = x.split()\n",
    "        x = [stemmer.lemmatize(word) for word in x]\n",
    "        x = ' '.join(x)\n",
    "        X[i] = x\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(X):\n",
    "    X = string_processing(X)\n",
    "\n",
    "    stopword = stopwords.words('english')\n",
    "    stopword.remove('no')\n",
    "    stopword.remove('not')\n",
    "    vectorizer = CountVectorizer(stop_words=stopword)\n",
    "    X = vectorizer.fit_transform(X).toarray()\n",
    "    print(X.shape)\n",
    "\n",
    "    tfidfconverter = TfidfTransformer()\n",
    "    X = tfidfconverter.fit_transform(X).toarray()\n",
    "    svd = TruncatedSVD(n_components=1500)\n",
    "    X = svd.fit_transform(X)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BND\n",
      "BRL\n",
      "CZK\n",
      "FJD\n",
      "ILS\n",
      "KHR\n",
      "MNT\n",
      "NZD\n",
      "PHP\n",
      "PLN\n",
      "RUB\n",
      "TRY\n",
      "TWD\n",
      "VND\n",
      "(1718, 77400)\n"
     ]
    }
   ],
   "source": [
    "exchange_rates, currencies = read_exchange_rate_csv('exchangerates/exchangeRateMatrix.csv')\n",
    "X, Y = read_news('news/',exchange_rates)\n",
    "X = pre_processing(X)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeCSV(X,Y) :\n",
    "    tmp = np.hstack((X,Y))\n",
    "    data_df = DataFrame(tmp)\n",
    "    data_df.to_csv('datas.csv',header= False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeCSV(X,Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
